{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:23:49.220458Z",
     "start_time": "2025-05-06T01:23:48.780965Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, requests, math\n",
    "import os, dotenv, logging\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    level = logging.INFO,\n",
    "    format = '[%(levelname)s] %(asctime)s — %(message)s',\n",
    "    datefmt = '%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6234d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:23:49.223550Z",
     "start_time": "2025-05-06T01:23:49.221543Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the user_id, user_agent and cookie for douban.env\n",
    "dotenv.load_dotenv(\"../personal_envs/personal-book-review-scraper_douban.env\", override=True)\n",
    "user_id, user_agent, cookie = os.getenv(\"user_id\"), os.getenv(\"user_agent\"), os.getenv(\"cookie\")\n",
    "\n",
    "# generate the header\n",
    "header = {'User-Agent': user_agent, 'Cookie': cookie}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa0519-3b24-42c4-bce8-1b7fbc2f5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether can login successful\n",
    "test_url = 'https://www.douban.com/'\n",
    "web_data = requests.get(test_url, headers = header)\n",
    "soup = BeautifulSoup(web_data.text, features = \"lxml\")\n",
    "\n",
    "if soup.select_one('li.nav-user-account span'):\n",
    "    print(f\"Login successful. Username: {soup.select_one('li.nav-user-account span').text.replace('的账号', '').strip()}\")\n",
    "else:\n",
    "    raise ValueError(\"Login failed. Please check if the cookie is valid or has expired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608ba56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:23:49.227717Z",
     "start_time": "2025-05-06T01:23:49.224086Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the function of parse each book in one page\n",
    "def get_books_info_from_page(url, header):\n",
    "    \n",
    "    # create the dataframe for all used columns\n",
    "    page_data = pd.DataFrame(columns = ['Name', 'Pub_info', 'Date', 'Star', 'Tag', 'Comment', 'Website'])\n",
    "    \n",
    "    # get the web data from the url\n",
    "    web_data = requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(web_data.text, features = \"lxml\")\n",
    "\n",
    "    # parse by each book\n",
    "    for div in soup.select('div.info'):\n",
    "        if div.find('a').contents[0] == '读书主页':\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                # get the feature of each book\n",
    "                book_name = str(div.find('a').contents[0]).replace('  ','').replace('\\n','')\n",
    "                book_pubinfo = str(div.select('div.pub')).split('>')[1].split('<')[0].replace('  ','').replace('\\n','')\n",
    "                book_date = str(div.select('span.date')).split('>')[1].split('<')[0][:10]\n",
    "                book_comment = str(div.find('p')).split('>')[1].split('<')[0].replace('  ','').replace('\\n','')\n",
    "                book_tag = str(div.select('span.tags')).split('>')[1].split('<')[0][4:]\n",
    "                book_star = [i for i in range(1, 6) if div.select(f'span.rating{i}-t')][0]\n",
    "                book_website = re.search(r'href=\"([^\"]+)\"', str(div.select('a')[0])).group(1)\n",
    "                \n",
    "                # combine the features in one row\n",
    "                book_parse = pd.DataFrame({'Name': book_name, 'Pub_info': book_pubinfo, \n",
    "                                           'Date': book_date, 'Star': book_star, \n",
    "                                           'Tag': book_tag, 'Comment': book_comment, \n",
    "                                           'Website': book_website}, \n",
    "                                           index=[0])\n",
    "                \n",
    "                # concat the book info together\n",
    "                page_data = pd.concat([page_data, book_parse], ignore_index = True)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                # print 'error' if wrong\n",
    "                print(book_name, 'parse error')\n",
    "                \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ab2a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:24:24.883116Z",
     "start_time": "2025-05-06T01:23:49.228679Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the data for all dataframe\n",
    "data = pd.DataFrame(columns = ['Name', 'Pub_info', 'Date', 'Star', 'Tag', 'Comment', 'Website'])\n",
    "\n",
    "# get the number of books\n",
    "url = f'https://book.douban.com/people/{user_id}/collect?start=0&sort=time&rating=all&filter=all&mode=grid'\n",
    "web_data = requests.get(url, headers = header)\n",
    "soup = BeautifulSoup(web_data.text, features=\"lxml\")\n",
    "\n",
    "web_title = str(soup.title).split('>')[1].split('<')[0].replace('  ','').replace('\\n','')\n",
    "num_books = int(re.findall(r\"\\(\\s*\\+?(-?\\d+)\\s*\\)\", web_title)[0])\n",
    "print('Total book #:', num_books)\n",
    "\n",
    "# parse the book information by page\n",
    "for i in range(0, math.ceil(num_books / 15)+1): # num_books ange(0, math.ceil(num_books / 15)+1)\n",
    "    if i != math.ceil(num_books / 15):\n",
    "        print(f'Parsing books: {i*15+1} ~ {i*15+15}...')\n",
    "    else:\n",
    "        print(f'Parsing books: {i*15+1} ~ {num_books}...')\n",
    "    url = f'https://book.douban.com/people/{user_id}/collect?start={i*15}&sort=time&rating=all&filter=all&mode=grid'\n",
    "    book_info = get_books_info_from_page(url, header)\n",
    "    data = pd.concat([data, book_info], ignore_index = True)\n",
    "    \n",
    "print('Finish parsing all books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397879e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:24:24.900471Z",
     "start_time": "2025-05-06T01:24:24.885171Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean the final parsed data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date').reset_index(drop=True)\n",
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797ecaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:24:25.061696Z",
     "start_time": "2025-05-06T01:24:24.901748Z"
    }
   },
   "outputs": [],
   "source": [
    "# export the data to a excel file\n",
    "data_saved_path = \"book_review_sample.xlsx\"\n",
    "data.to_excel(data_saved_path)\n",
    "logging.info(f\"XLSX generated — Path: {data_saved_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da107bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
